{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c277c688",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);\">\n",
    "    <h1 style=\"color: #FFF; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);\">üí¨ | Step 2: Customize The Tone & Style With SFT </h1>\n",
    "        <p style=\"font-size: 16px; line-height: 1.6;\">\n",
    "            We used few shot examples to prompt-engineer a better tone. We used RAG to ground responses in our data. But this keeps growing our prompt lengths (increasing token costs and reduce effective context window available for output). How can we improve the tone and style of our bot with _more examples_ and shorter prompt length?\n",
    "        </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a8518",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; padding: 5px; height: 40px; background: linear-gradient(90deg, #333333 0%, #777777 50%, #BBBBBB 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;\">\n",
    "    Step 1 : Understand Zava Scenarios\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"display: flex; align-items: center; justify-content: left; padding: 5px; height: 40px; background: linear-gradient(90deg, #7873f5 0%, #ff6ec4 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;\">\n",
    "    Step 3: Be More Cost-Effective With Distillation\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e60caa",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Check Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495e2ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: gpt-4.1\n",
      "Using API Version: 2025-02-01-preview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "model_name = \"gpt-4.1\"\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-02-01-preview\")\n",
    "\n",
    "if not openai_key or not openai_endpoint:\n",
    "    print(\"Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "print(\"Using Model:\", model_name)\n",
    "print(\"Using API Version:\", api_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae8408",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Validate Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d21d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Training and Validation datafiles\n",
    "\n",
    "\n",
    "training_file = \"../data/basic_sft_training.jsonl\" \n",
    "validation_file = \"../data/basic_sft_validation.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9fb115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in training set: 40\n",
      "First example in training set:\n",
      "{'role': 'system', 'content': 'You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store.'}\n",
      "{'role': 'user', 'content': 'Is water-based polyurethane better than oil-based?'}\n",
      "{'role': 'assistant', 'content': 'ü§î Both have benefits! Water-based at $43 dries faster, oil-based at $34 gives amber tone. Which matters most?'}\n",
      "\n",
      "Number of examples in validation set: 10\n",
      "First example in validation set:\n",
      "{'role': 'system', 'content': 'You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store.'}\n",
      "{'role': 'user', 'content': \"What's good for painting wrought iron?\"}\n",
      "{'role': 'assistant', 'content': 'üîß Metal Primer is perfect for wrought iron! Metal Primer at $36 first, then any topcoat. Existing rust present?'}\n"
     ]
    }
   ],
   "source": [
    "# Run preliminary checks\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the training set\n",
    "with open(training_file, 'r', encoding='utf-8') as f:\n",
    "    training_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Training dataset stats\n",
    "print(\"Number of examples in training set:\", len(training_dataset))\n",
    "print(\"First example in training set:\")\n",
    "for message in training_dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Load the validation set\n",
    "with open(validation_file, 'r', encoding='utf-8') as f:\n",
    "    validation_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Validation dataset stats\n",
    "print(\"\\nNumber of examples in validation set:\", len(validation_dataset))\n",
    "print(\"First example in validation set:\")\n",
    "for message in validation_dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345304de",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Assess Token Counts For Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d106f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../data/basic_sft_training.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 62, 78\n",
      "mean / median: 68.375, 68.0\n",
      "p5 / p95: 65.0, 71.1\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 19, 30\n",
      "mean / median: 23.75, 24.0\n",
      "p5 / p95: 20.9, 26.1\n",
      "**************************************************\n",
      "Processing file: ../data/basic_sft_validation.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 66, 76\n",
      "mean / median: 69.4, 69.0\n",
      "p5 / p95: 66.9, 73.3\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 22, 28\n",
      "mean / median: 24.4, 24.0\n",
      "p5 / p95: 22.9, 26.2\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Validate token counts\n",
    "\n",
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\") # default encoding for gpt-4o models. This requires the latest version of tiktoken to be installed.\n",
    "\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "files = [training_file, validation_file]\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    total_tokens = []\n",
    "    assistant_tokens = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        messages = ex.get(\"messages\", {})\n",
    "        total_tokens.append(num_tokens_from_messages(messages))\n",
    "        assistant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print_distribution(total_tokens, \"total tokens\")\n",
    "    print_distribution(assistant_tokens, \"assistant tokens\")\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023e173",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Upload Fine-Tuning Data To Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9d2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure OpenAI Client\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "  api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11fc1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-df310fefd1cc48148327305ec11e246a\n",
      "Validation file ID: file-0ad0d42ef65f4865abba1189098f7bca\n"
     ]
    }
   ],
   "source": [
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "# Note: You can visit the Azure AI Foundry Portal - and look under your Azure AI Project's 'Data Files' tab to see the uploaded files.\n",
    "\n",
    "training_response = client.files.create(\n",
    "    file = open(training_file, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response.id\n",
    "\n",
    "validation_response = client.files.create(\n",
    "    file = open(validation_file, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response.id\n",
    "\n",
    "print(\"Training file ID:\", training_file_id)\n",
    "print(\"Validation file ID:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bee32c",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Submit The Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ddf91d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'invalidPayload', 'message': 'The specified file reference must point to a completed file import.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Submit fine-tuning training job\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Note that the model you specify here must be one that can be fine-tuned.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Currently gpt-4.1 can be fine-tuned only in Sweden Central and North Central US\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# See: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tuning?tabs=azure-openai&pivots=programming-language-python#prerequisites\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfine_tuning\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_file_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_file_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Enter base model name. Note that in Azure OpenAI the model name contains dashes and cannot contain dot/period characters.\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m105\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# seed parameter controls reproducibility of the fine-tuning job. If no seed is specified one will be generated automatically.\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_rate_multiplier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Higher LR for faster training\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_epochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced epochs (default is often overkill)\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Larger batch for stability\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m job_id = response.id\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# You can use the job ID to monitor the status of the fine-tuning job.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# The fine-tuning job will take some time to start and complete.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/ignite25-PDY123-learn-how-to-observe-manage-and-scale-agentic-ai-apps-using-azure/.venv/lib/python3.13/site-packages/openai/resources/fine_tuning/jobs/jobs.py:158\u001b[39m, in \u001b[36mJobs.create\u001b[39m\u001b[34m(self, model, training_file, hyperparameters, integrations, metadata, method, seed, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     63\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m     79\u001b[39m ) -> FineTuningJob:\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    Creates a fine-tuning job which begins the process of creating a new model from\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    a given dataset.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/fine_tuning/jobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_file\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhyperparameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mintegrations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegrations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmethod\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuffix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation_file\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjob_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJobCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFineTuningJob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/ignite25-PDY123-learn-how-to-observe-manage-and-scale-agentic-ai-apps-using-azure/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/ignite25-PDY123-learn-how-to-observe-manage-and-scale-agentic-ai-apps-using-azure/.venv/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'invalidPayload', 'message': 'The specified file reference must point to a completed file import.'}}"
     ]
    }
   ],
   "source": [
    "# Submit fine-tuning training job\n",
    "# Note that the model you specify here must be one that can be fine-tuned.\n",
    "# Currently gpt-4.1 can be fine-tuned only in Sweden Central and North Central US\n",
    "# See: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tuning?tabs=azure-openai&pivots=programming-language-python#prerequisites\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=\"gpt-4.1\", # Enter base model name. Note that in Azure OpenAI the model name contains dashes and cannot contain dot/period characters.\n",
    "    seed = 105,  # seed parameter controls reproducibility of the fine-tuning job. If no seed is specified one will be generated automatically.\n",
    "    hyperparameters={\n",
    "        \"learning_rate_multiplier\": 2.0,  # Higher LR for faster training\n",
    "        \"n_epochs\": 3,  # Reduced epochs (default is often overkill)\n",
    "        \"batch_size\": 16,  # Larger batch for stability\n",
    "    }\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tuning job.\n",
    "# The fine-tuning job will take some time to start and complete.\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.id)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468e4f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Track Fine-Tuning Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job ftjob-c387da6dc41344a48a2c00cdb543f9ff finished with status: succeeded\n",
      "Checking other fine-tune jobs for this resource.\n",
      "Found 20 fine-tune jobs.\n"
     ]
    }
   ],
   "source": [
    "# Track training status\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the status of our fine-tuning job.\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "status = response.status\n",
    "\n",
    "# If the job isn't done yet, poll it every 10 seconds.\n",
    "while status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(10)\n",
    "\n",
    "    response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    print(response.model_dump_json(indent=2))\n",
    "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "    status = response.status\n",
    "    print(f'Status: {status}')\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f'Fine-tuning job {job_id} finished with status: {status}')\n",
    "\n",
    "# List all fine-tuning jobs for this resource.\n",
    "print('Checking other fine-tune jobs for this resource.')\n",
    "response = client.fine_tuning.jobs.list()\n",
    "print(f'Found {len(response.data)} fine-tune jobs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81343ca5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. List Fine-Tuning Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"ftevent-ddad2e8381574b9bb5115a949deade7d\",\n",
      "      \"created_at\": 1759398914,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Training tokens billed: 8000\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-086c7c89dc9a45ddb76150dcdd983c36\",\n",
      "      \"created_at\": 1759398914,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed results file: file-963292befa624ed2ade8c587609042bb\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-b40ea3b38f5e483b840fdb429334b298\",\n",
      "      \"created_at\": 1759398911,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Model Evaluation Passed.\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-276e0b962fed4fd6b500d3e2a2754355\",\n",
      "      \"created_at\": 1759398854,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Job succeeded.\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-008de0194405fd9008de0194405fd900\",\n",
      "      \"created_at\": 1759396538,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 8: training loss=4.689654350280762\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 8,\n",
      "        \"train_loss\": 4.689654350280762,\n",
      "        \"train_mean_token_accuracy\": 0.3235294222831726,\n",
      "        \"valid_loss\": 4.310887895311628,\n",
      "        \"valid_mean_token_accuracy\": 0.3595238095238095,\n",
      "        \"full_valid_loss\": 4.2378693927418105,\n",
      "        \"full_valid_mean_token_accuracy\": 0.375\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808de01943c33bb808de01943c33bb80\",\n",
      "      \"created_at\": 1759396531,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 1: training loss=7.289500713348389\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 1,\n",
      "        \"train_loss\": 7.289500713348389,\n",
      "        \"train_mean_token_accuracy\": 0.3112744987010956\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-f19f743f433344a2af07e9b1e1517e03\",\n",
      "      \"created_at\": 1759396530,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created results file: file-963292befa624ed2ade8c587609042bb\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-1bdba78410a34a40b8c49b3657b33bef\",\n",
      "      \"created_at\": 1759395643,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Training started.\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-4706d287f9d142cca7ef784f5237eb54\",\n",
      "      \"created_at\": 1759394994,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Job queued, waiting for GPUs.\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-d1679f1ce48c497f9329e076a8d86c2c\",\n",
      "      \"created_at\": 1759394639,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Data Import started.\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": true,\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56873a47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509896ba",
   "metadata": {},
   "source": [
    "### 8. List Fine-Tuning Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088ebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"ftchkpt-e30cc21551b24afe8e4a8510522ec286\",\n",
      "      \"created_at\": 1759396725,\n",
      "      \"fine_tuned_model_checkpoint\": \"gpt-4.1-2025-04-14.ft-c387da6dc41344a48a2c00cdb543f9ff\",\n",
      "      \"fine_tuning_job_id\": \"ftjob-c387da6dc41344a48a2c00cdb543f9ff\",\n",
      "      \"metrics\": {\n",
      "        \"full_valid_loss\": 4.2378693927418105,\n",
      "        \"full_valid_mean_token_accuracy\": 0.375,\n",
      "        \"step\": 8.0,\n",
      "        \"train_loss\": 4.689654350280762,\n",
      "        \"train_mean_token_accuracy\": 0.3235294222831726,\n",
      "        \"valid_loss\": 4.310887895311628,\n",
      "        \"valid_mean_token_accuracy\": 0.3595238095238095\n",
      "      },\n",
      "      \"object\": \"fine_tuning.job.checkpoint\",\n",
      "      \"step_number\": 8\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftchkpt-3027a2f97d01454f87de5f5a561fda4d\",\n",
      "      \"created_at\": 1759396643,\n",
      "      \"fine_tuned_model_checkpoint\": \"gpt-4.1-2025-04-14.ft-c387da6dc41344a48a2c00cdb543f9ff:ckpt-step-6\",\n",
      "      \"fine_tuning_job_id\": \"ftjob-c387da6dc41344a48a2c00cdb543f9ff\",\n",
      "      \"metrics\": {\n",
      "        \"full_valid_loss\": 4.507203752344305,\n",
      "        \"full_valid_mean_token_accuracy\": 0.36363636363636365,\n",
      "        \"step\": 6.0,\n",
      "        \"train_loss\": 4.790422439575195,\n",
      "        \"train_mean_token_accuracy\": 0.35180723667144775,\n",
      "        \"valid_loss\": null,\n",
      "        \"valid_mean_token_accuracy\": null\n",
      "      },\n",
      "      \"object\": \"fine_tuning.job.checkpoint\",\n",
      "      \"step_number\": 6\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftchkpt-b7545f5200ca420cad932102abefc082\",\n",
      "      \"created_at\": 1759396561,\n",
      "      \"fine_tuned_model_checkpoint\": \"gpt-4.1-2025-04-14.ft-c387da6dc41344a48a2c00cdb543f9ff:ckpt-step-3\",\n",
      "      \"fine_tuning_job_id\": \"ftjob-c387da6dc41344a48a2c00cdb543f9ff\",\n",
      "      \"metrics\": {\n",
      "        \"full_valid_loss\": 5.935820485606338,\n",
      "        \"full_valid_mean_token_accuracy\": 0.3333333333333333,\n",
      "        \"step\": 3.0,\n",
      "        \"train_loss\": 6.424859046936035,\n",
      "        \"train_mean_token_accuracy\": 0.301980197429657,\n",
      "        \"valid_loss\": null,\n",
      "        \"valid_mean_token_accuracy\": null\n",
      "      },\n",
      "      \"object\": \"fine_tuning.job.checkpoint\",\n",
      "      \"step_number\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": false,\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.checkpoints.list(job_id)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3e13f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42313dcc",
   "metadata": {},
   "source": [
    "### 9. Retrieve Fine-Tuned Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"ftjob-c387da6dc41344a48a2c00cdb543f9ff\",\n",
      "  \"created_at\": 1759394308,\n",
      "  \"error\": null,\n",
      "  \"fine_tuned_model\": \"gpt-4.1-2025-04-14.ft-c387da6dc41344a48a2c00cdb543f9ff\",\n",
      "  \"finished_at\": 1759398914,\n",
      "  \"hyperparameters\": {\n",
      "    \"batch_size\": 16,\n",
      "    \"learning_rate_multiplier\": 2.0,\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"model\": \"gpt-4.1-2025-04-14\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": null,\n",
      "  \"result_files\": [\n",
      "    \"file-963292befa624ed2ade8c587609042bb\"\n",
      "  ],\n",
      "  \"seed\": 105,\n",
      "  \"status\": \"succeeded\",\n",
      "  \"trained_tokens\": 10485,\n",
      "  \"training_file\": \"file-7fae087b7ad3415a976c647af3a074bd\",\n",
      "  \"validation_file\": \"file-3eaf80c42c344ff5a7a7bd992fc1de15\",\n",
      "  \"estimated_finish\": 1759395748,\n",
      "  \"integrations\": null,\n",
      "  \"metadata\": null,\n",
      "  \"method\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve fine_tuned_model name\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n",
    "fine_tuned_model = response.fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39afff9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 10. Deploy Fine-Tuned Model For Testing\n",
    "\n",
    "For now - we deployed this manually from the Azure AI Foundry Portal - using the **developer tier** which allows us to test our fine-tuned model for the cost of just inferencing. Once we deploy it, we can try it out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85182ed",
   "metadata": {},
   "source": [
    "> Prompt 1: What kind of paint should I buy for my outdoor deck?\n",
    "\n",
    "```txt\n",
    "ü™µ Deck protection options! Semi-Transparent Deck Stain at 38 enhances wood grain, or Deck & Fence Stain at 36 for UV protection?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892334a4",
   "metadata": {},
   "source": [
    "> Prompt 2: I'm painting over rust - what spray paint should I use?\n",
    "\n",
    "üëç Right choice! Rust Prevention Spray at $13 applies directly over rust with long-lasting protection. Primer recommendation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39c0bb",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "\n",
    "In both the examples above we can note that the response now accurately follows our Zava guidelines for \"polite, factual and helpful\"\n",
    "- Every response starts with an emoji\n",
    "- The first sentence is always an acknowledgement of the user (\"polite\")\n",
    "- The next sentence is always an informative segment (\"factual\")\n",
    "- The final senteance is always an offer to follow up (\"helpful\")\n",
    "\n",
    "And note that we have the succinct responses we were looking for _without adding few-shot examples_, making the prompts shorter and thus saving both token costs and processing latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08deb97",
   "metadata": {},
   "source": [
    "---\n",
    "### Teardown\n",
    "\n",
    "Once you are done with this lab, don't forget to tear down the infrastructure. The developer tier model will be torn down automatically (after 24 hours?) but it is better to proactively delete the resource group and release all model quota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee386b6",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: left; padding: 5px; height: 40px; background: linear-gradient(90deg, #7873f5 0%, #ff6ec4 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;\">\n",
    "    Next: Be More Cost-Effective With Distillation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54567f40",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
